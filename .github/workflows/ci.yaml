name: Build MLC LLM Runtime

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash -l {0}

    steps:

    # 1️⃣ Checkout repo + submodules
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        submodules: recursive

    # 2️⃣ Setup Conda
    - name: Setup Miniconda
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-update-conda: true
        auto-activate-base: false
        activate-environment: mlc-chat-venv
        python-version: 3.13

    # 3️⃣ Create build env
    - name: Create build environment
      run: |
        conda create -y -n mlc-chat-venv -c conda-forge \
          "cmake>=3.24" \
          rust \
          git \
          python=3.13

    # 4️⃣ Generate config (non-interactive)
    - name: Generate CMake config
      run: |
        conda activate mlc-chat-venv
        mkdir -p build
        cd build

        printf "\nn\nn\nn\nn\nn\n" | python ../cmake/gen_cmake_config.py

    # 5️⃣ Build runtime
    - name: Build mlc_llm runtime
      run: |
        conda activate mlc-chat-venv
        cd build
        cmake ..
        make -j$(nproc)

    # ✅ NEW FIX STEP
    - name: Install TVM Python bindings
      run: |
        conda activate mlc-chat-venv
        cd 3rdparty/tvm/python
        pip install -e .

    # 6️⃣ Install mlc_llm Python package
    - name: Install mlc_llm Python package
      run: |
        conda activate mlc-chat-venv
        cd python
        pip install -e .

    # 7️⃣ Validate build
    - name: Validate installation
      run: |
        conda activate mlc-chat-venv

        echo "=== Build artifacts ==="
        ls -l ./build/ | grep -E "libmlc_llm|libtvm_runtime" || true

        echo "=== CLI test ==="
        mlc_llm chat -h

        echo "=== Python import test ==="
        python -c "import mlc_llm; print(mlc_llm)"
