name: linux-gpu-build

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  Build-and-Test-GPU:
    runs-on: self-hosted

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Clean and Prep Environment
        shell: bash
        run: |
          # Ensure conda is initialized
          source ~/miniconda3/etc/profile.d/conda.sh || source /home/ubuntu/miniconda3/etc/profile.d/conda.sh
          
          conda env remove -n mlc-chat-venv -y || true
          conda create -n mlc-chat-venv python=3.13 -y
          # Added llvmdev (Required for TVM)
          conda install -n mlc-chat-venv -c conda-forge -y \
            "cmake>=3.24" rust git numpy scipy psutil decorator attrs cloudpickle llvmdev=18
          
      - name: Build TVM (with CUDA 12.2)
        shell: bash
        run: |
          source ~/miniconda3/etc/profile.d/conda.sh || source /home/ubuntu/miniconda3/etc/profile.d/conda.sh
          conda activate mlc-chat-venv
          
          cd 3rdparty/tvm
          rm -rf build && mkdir build && cd build
          
          # Added USE_LLVM=ON and CUDA Architecture for A10G (86)
          cmake .. \
            -DUSE_CUDA=ON \
            -DUSE_CUBLAS=ON \
            -DUSE_CUTLASS=ON \
            -DUSE_LLVM=ON \
            -DUSE_CUDNN=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DCMAKE_CUDA_ARCHITECTURES=86
            
          # Using nproc-1 to leave some RAM for the OS during linking
          make -j$(($(nproc)-1))

      - name: Build mlc-llm core
        shell: bash
        run: |
          source ~/miniconda3/etc/profile.d/conda.sh || source /home/ubuntu/miniconda3/etc/profile.d/conda.sh
          conda activate mlc-chat-venv
          
          rm -rf build && mkdir build && cd build
          cmake .. \
            -DUSE_CUDA=ON \
            -DUSE_CUBLAS=ON \
            -DUSE_CUTLASS=ON \
            -DUSE_LLVM=ON \
            -DBUILD_SHARED_LIBS=ON \
            -DCMAKE_CUDA_ARCHITECTURES=86
          make -j$(($(nproc)-1))

      - name: Install MLC Python Package & Set Env
        shell: bash
        run: |
          source ~/miniconda3/etc/profile.d/conda.sh || source /home/ubuntu/miniconda3/etc/profile.d/conda.sh
          conda activate mlc-chat-venv
          pip install -e python
          
          echo "TVM_HOME=$GITHUB_WORKSPACE/3rdparty/tvm" >> $GITHUB_ENV
          echo "PYTHONPATH=$GITHUB_WORKSPACE/python:$GITHUB_WORKSPACE/3rdparty/tvm/python:$PYTHONPATH" >> $GITHUB_ENV
          echo "LD_LIBRARY_PATH=$GITHUB_WORKSPACE/build:$GITHUB_WORKSPACE/3rdparty/tvm/build:/usr/local/cuda-12.2/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH" >> $GITHUB_ENV

      - name: Verify GPU and Installation
        shell: bash
        run: |
          source ~/miniconda3/etc/profile.d/conda.sh || source /home/ubuntu/miniconda3/etc/profile.d/conda.sh
          conda activate mlc-chat-venv
          python -c "import tvm; from tvm import cuda; print('TVM Version:', tvm.__version__); print('CUDA Device 0 exists:', cuda(0).exist)"
          mlc_llm chat --help
